# -*- coding: utf-8 -*-
"""Homework 4 Testing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C-cw5zL1MRDRd8x5Wd-T95g0oho361zq
"""

import tensorflow_datasets as tfds
import tensorflow as tf
import numpy as np
import datetime as datetime
train_ds, test_ds = tfds.load('mnist', split=['train', 'test'], as_supervised=True)


def prepare_mnist_data(mnist, subtask):
  global currentsubtask
  #flatten the images into vectors
  mnist = mnist.map(lambda img, target: (tf.reshape(img, (-1,)), target))
  #convert data from uint8 to float32
  mnist = mnist.map(lambda img, target: (tf.cast(img, tf.float32), target))
  #sloppy input normalization, just bringing image values from range [0, 255] to [-1, 1]
  mnist = mnist.map(lambda img, target: ((img/128.)-1., target))
  #Zipping, so that there are two mnist sets in one dataset
  zipped_ds = tf.data.Dataset.zip((mnist.shuffle(2000), mnist.shuffle(2000)))
  #differneciating between subtask 1 and subtask 2
  if subtask == 1:
    zipped_ds = zipped_ds.map(lambda x1, x2: (x1[0], x2[0], (x1[1]+x2[1])>=5))
    zipped_ds = zipped_ds.map(lambda x1, x2, t: (x1,x2, tf.cast(t, tf.int32)))
    currentsubtask = 1
  if subtask==2:
    zipped_ds = zipped_ds.map(lambda x1, x2: (x1[0], x2[0], x1[1]-x2[1]))
    zipped_ds = zipped_ds.map(lambda x1, x2, target: (x1, x2, tf.one_hot(target, depth=19)))
    currentsubtask = 2
  zipped_ds = zipped_ds.batch(32)
  # prefetch
  zipped_ds = zipped_ds.prefetch(tf.data.AUTOTUNE)
  return zipped_ds

train_dataset = prepare_mnist_data(train_ds, subtask=2)
test_dataset = prepare_mnist_data(test_ds, subtask=2)

# check the contents of the dataset
for img1, img2, label in train_dataset:
    print(img1.shape, img2.shape, label.shape)
    break

#from tensorflow.keras.layers import Dense


class homework4model(tf.keras.Model):

    #constructor -- differenciating between the subtasks
    def __init__(self, optimizer):
        super().__init__()  
        self.optimizer = optimizer 
        if currentsubtask == 1:
          self.loss_function = tf.keras.losses.BinaryCrossentropy() #binary cross entropy for binary task (bigger than 5 yes or no)
          self.metrics_list = [tf.keras.metrics.BinaryAccuracy(),
                             tf.keras.metrics.Mean(name="loss")]
        if currentsubtask == 2:
          self.loss_function = tf.keras.losses.CategoricalCrossentropy() #categorical cross entropy for classification task (possible digits from [-9;9])
          self.metrics_list = [tf.keras.metrics.CategoricalAccuracy(),
                             tf.keras.metrics.Mean(name="loss")] 

        #network layers (two output layers for different tasks)
        self.dense1 = tf.keras.layers.Dense(128, activation=tf.nn.relu)
        self.dense2 = tf.keras.layers.Dense(128, activation=tf.nn.relu) 
        self.dense3 = tf.keras.layers.Dense(128, activation=tf.nn.relu) 
        self.out_layer1 = tf.keras.layers.Dense(1,activation=tf.nn.sigmoid)
        self.out_layer2 = tf.keras.layers.Dense(19,activation=tf.nn.softmax)
        
    # structure of input
    def call(self, images, training=False):
        img1, img2 = images
        
        img1_x = self.dense1(img1)
        img1_x = self.dense2(img1_x)
        
        img2_x = self.dense1(img2)
        img2_x = self.dense2(img2_x)
        
        #concatinate both images and put into final layers
        combined_x = tf.concat([img1_x, img2_x ], axis=1)
        combined_x = self.dense3(combined_x)
        if currentsubtask == 1:
          return self.out_layer1(combined_x)
        if currentsubtask == 2:
          return self.out_layer2(combined_x)

 #metrics property -- as shown in the flipped classroom
    @property
    def metrics(self):
        return self.metrics_list
        # return a list with all metrics in the model

    # reset all metrics objects
    def reset_metrics(self):
        for metric in self.metrics:
            metric.reset_states()

    #train step method
    @tf.function
    def train_step(self, data):
        img1, img2, label = data
        
        with tf.GradientTape() as tape:
            output = self((img1, img2), training=True)
            loss = self.loss_function(label, output)
            
        gradients = tape.gradient(loss, self.trainable_variables)
        
        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))
        
        # update the state of the metrics according to loss
        self.metrics[0].update_state(label, output)
        self.metrics[1].update_state(loss)
        
        # return a dictionary with metric names as keys and metric results as values
        return {m.name : m.result() for m in self.metrics}

    #test_step method
    @tf.function
    def test_step(self, data):
        img1, img2, label = data
        # same as train step (without parameter updates)
        output = self((img1, img2), training=False)
        loss = self.loss_function(label, output)
        self.metrics[0].update_state(label, output)
        self.metrics[1].update_state(loss)
        
        return {m.name : m.result() for m in self.metrics}

#copy from flipped classroom

def create_summary_writers(config_name):
    
    # Define where to save the logs
    # along with this, you may want to save a config file with the same name so you know what the hyperparameters were used
    # alternatively make a copy of the code that is used for later reference
    
    current_time = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")

    train_log_path = f"logs/{config_name}/{current_time}/train"
    val_log_path = f"logs/{config_name}/{current_time}/val"

    # log writer for training metrics
    train_summary_writer = tf.summary.create_file_writer(train_log_path)

    # log writer for validation metrics
    val_summary_writer = tf.summary.create_file_writer(val_log_path)
    
    return train_summary_writer, val_summary_writer

train_summary_writer, val_summary_writer = create_summary_writers(config_name="RUN1")

import tqdm
def training_loop(model, train_ds, test_ds, start_epoch,
                  epochs, train_summary_writer, 
                  val_summary_writer, save_path):

    #iterate over epochs
    for e in range(start_epoch, epochs):

        #train steps on all batches in the training data
        for data in tqdm.tqdm(train_ds, position=0, leave=True):
            metrics = model.train_step(data)

        #log and print training metrics

        with train_summary_writer.as_default():
            # for scalar metrics:
            for metric in model.metrics:
                    tf.summary.scalar(f"{metric.name}", metric.result(), step=e)
        
        #print the metrics
        print([f"{key}: {value.numpy()}" for (key, value) in metrics.items()])
        
        #reset metric objects
        model.reset_metrics()


        #evaluate on validation data
        for data in test_ds:
            metrics = model.test_step(data)
        

        #log validation metrics

        with val_summary_writer.as_default():
            # for scalar metrics:
            for metric in model.metrics:
                    tf.summary.scalar(f"{metric.name}", metric.result(), step=e)
            
        print([f"val_{key}: {value.numpy()}" for (key, value) in metrics.items()])
        #reset metric objects
        model.reset_metrics()
        
    #save model weights if save_path is given
    if save_path:
        model.save_weights(save_path)

# 1. instantiate model
model1 = homework4model(tf.keras.optimizers.SGD(0.01))
model2 = homework4model(tf.keras.optimizers.Adam())

# 2. choose a path to save the weights
save_path = "trained_model_RUN1"

# 2. pass arguments to training loop function
# training loop with SGD optimizer 
if currentsubtask==2:
  training_loop(model=model1,
      train_ds=train_dataset,
      test_ds=test_dataset,
      start_epoch=0,
      epochs=10,
      train_summary_writer=train_summary_writer,
      val_summary_writer=val_summary_writer,
      save_path=save_path)

# training loop with Adam optimizer 
if currentsubtask==1:
  training_loop(model=model2,
      train_ds=train_dataset,
      test_ds=test_dataset,
      start_epoch=0,
      epochs=10,
      train_summary_writer=train_summary_writer,
      val_summary_writer=val_summary_writer,
      save_path=save_path)