{"cells":[{"cell_type":"markdown","metadata":{},"source":[" # Homework 5 "]},{"cell_type":"markdown","metadata":{},"source":[" ### 2 . CIFAR10 - Classification "]},{"cell_type":"markdown","metadata":{},"source":["The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n","\n","Feature structure: \n","id: Text(dtype=object)\n","image: Image(shape=(32,32,3), dtype=uint8)\n","label: ClassLabel(dtype=int64, num_classes=10)"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import tensorflow_datasets as tfds\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","from matplotlib import image\n","import numpy as np\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["(train_ds, test_ds), ds_info = tfds.load('cifar10', split=['train', 'test'], as_supervised=True, with_info = True)\n","#print(ds_info)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def prepare_cifar10_data(cifar, batchsize = 32):\n","  '''\n","  This function prepares the dataset for usage in a learning model\n","\n","  Args:\n","  cifar --- The cifar-10 dataset \n","  batchsize --- The batch size (default:32)\n","\n","  Returns:\n","  cifar --- The processed dataset\n","\n","  '''\n","  #convert data from uint8 to float32 \n","  cifar = cifar.map(lambda img, target: (tf.cast(img, tf.float32), target))\n","  #sloppy input normalization, just bringing image values from range [0, 255] to [-1, 1]\n","  cifar = cifar.map(lambda img, target: ((img/128.)-1., target))\n","  #create one-hot targets\n","  cifar = cifar.map(lambda img, target: (img, tf.one_hot(target, depth=10)))\n","  #cache this progress in memory\n","  cifar = cifar.cache()\n","  #shuffle, batch, prefetch\n","  cifar = cifar.shuffle(1000)\n","  cifar = cifar.batch(batchsize)\n","  cifar= cifar.prefetch(tf.data.AUTOTUNE)\n","  #return preprocessed dataset\n","  return cifar\n","\n","\n","train_dataset = train_ds.apply(prepare_cifar10_data)\n","test_dataset = test_ds.apply(prepare_cifar10_data)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tfds.visualization.show_examples(test_ds, ds_info)"]},{"cell_type":"markdown","metadata":{},"source":["## Creating the Model"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["class ConvModel(tf.keras.Model):\n","    def __init__(self, layers_list, optimizer):\n","        super().__init__()\n","\n","        self.loss_function = tf.keras.losses.CategoricalCrossentropy()\n","        self.optimizer = optimizer\n","\n","        #instantiating layers through given list with layer objects\n","        self.con_layers = layers_list\n","        self.out = tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n","\n","        #metric objects to keep track of loss and accuracy\n","        self.metrics_list = [tf.keras.metrics.Mean(name=\"loss\"), tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\")]\n","\n","    @tf.function\n","    def call(self, input):\n","        '''\n","        Propagates input through network\n","\n","        Args:\n","        input --- the network's input\n","\n","        Returns:\n","        out --- the network's output\n","        '''\n","        out = self.con_layers[0](input)\n","        for layer in self.con_layers[1:]:\n","            out = layer(out)\n","        out = self.out(out)\n","        return out \n","\n","    #metrics property \n","    @property\n","    def metrics(self):\n","        # returns a list with all metrics in the model\n","        return self.metrics_list\n","\n","\n","    def reset_metrics(self):\n","        # reset all metrics objects\n","        for metric in self.metrics:\n","            metric.reset_states()\n","\n","\n","    @tf.function\n","    def train_step(self, input):\n","        '''\n","        Updates the trainable variables according to the calculated gradients \n","\n","        Args:\n","        input --- the input on which the training step is executed\n","\n","        Returns:\n","        A dictionary of loss and accuracy metrices \n","        '''\n","        img, target = input\n","        \n","        with tf.GradientTape() as tape:\n","            #create networks prediction\n","            output = self(img, training=True)\n","            loss = self.loss_function(target, output)\n","        #calculating gradients    \n","        gradients = tape.gradient(loss, self.trainable_variables)\n","        \n","        #applying gradients and updating model\n","        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n","        \n","        # update the state of the metrics according to loss\n","        self.metrics[0].update_state(loss)\n","        self.metrics[1].update_state(target, output)\n","\n","        \n","        # return a dictionary with metric names as keys and metric results as values\n","        return [m.result() for m in self.metrics]\n","\n","    @tf.function\n","    def test_step(self, data):\n","        ''' \n","        Tests how model performs on data it wasn't trained on \n","\n","        Args:\n","        data --- test data\n","\n","        Returns:\n","        A dictionary of loss and accuracy metrices \n","        '''\n","        img, target = data \n","\n","        #predict output and calculate loss\n","        output = self(img, training=True)\n","        loss = self.loss_function(target, output)\n","\n","        # update the state of the metrics according to loss\n","        self.metrics[0].update_state(loss)\n","        self.metrics[1].update_state(target, output)\n","\n","        return [m.result() for m in self.metrics]\n","\n","\n","\n","\n","\n","\n","        \n"]},{"cell_type":"markdown","metadata":{},"source":["## Training the network"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def training_loop(model, train_data, test_data, epochs):\n","    ''' \n","    The training loop of our model\n","\n","    Args:\n","    model --- the model object \n","    epochs --- number of training steps\n","    '''\n","\n","    train_losses = []\n","    train_accuracies = []\n","    test_losses = []\n","    test_accuracies = []\n","\n","    #test metrics before training \n","    for data in test_data:\n","        test_loss, test_accuracy = model.test_step(data)\n","    test_losses.append(test_loss)\n","    test_accuracies.append(test_accuracy)\n","\n","    model.reset_metrics()\n","\n","    for epoch in range(epochs):\n","\n","        #print accuracy every epoch\n","        print(f'Epoch {str(epoch)}: Accuracy {test_accuracies[-1]}')\n","\n","        #train the network\n","        for data in train_data:\n","                train_loss, train_accuracy = model.train_step(data)\n","        train_losses.append(train_loss)\n","        train_accuracies.append(train_accuracy)\n","\n","        model.reset_metrics()\n","\n","        # test the network\n","        for data in test_dataset:\n","                test_loss, test_accuracy = model.test_step(data)\n","        test_losses.append(test_loss.numpy())\n","        test_accuracies.append(test_accuracy.numpy())\n","\n","        model.reset_metrics()\n","\n","    return train_losses, train_accuracies, test_losses, test_accuracies\n","\n"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0: Accuracy 0.09889999777078629\n","Epoch 1: Accuracy 0.4239000082015991\n","Epoch 2: Accuracy 0.5238000154495239\n","Epoch 3: Accuracy 0.5849000215530396\n","Epoch 4: Accuracy 0.6226000189781189\n","Epoch 5: Accuracy 0.640999972820282\n","Epoch 6: Accuracy 0.6682000160217285\n","Epoch 7: Accuracy 0.6448000073432922\n","Epoch 8: Accuracy 0.6761999726295471\n","Epoch 9: Accuracy 0.6775000095367432\n","Epoch 10: Accuracy 0.6967999935150146\n","Epoch 11: Accuracy 0.7041000127792358\n","Epoch 12: Accuracy 0.7152000069618225\n","Epoch 13: Accuracy 0.7096999883651733\n","Epoch 14: Accuracy 0.7324000000953674\n"]}],"source":["#training for 15 epochs\n","epochs = 15\n","\n","#same layer config as in lecture \n","layers_list = [tf.keras.layers.Conv2D(filters=24, kernel_size=3, padding='same', activation='relu'),\n","        tf.keras.layers.Conv2D(filters=24, kernel_size=3, padding='same', activation='relu'), \n","        tf.keras.layers.MaxPooling2D(pool_size=2, strides=2),\n","        tf.keras.layers.Conv2D(filters=48, kernel_size=3, padding='same', activation='relu'),\n","        tf.keras.layers.Conv2D(filters=48, kernel_size=3, padding='same', activation='relu'),\n","        tf.keras.layers.GlobalAvgPool2D()]\n","\n","#using Adam optimizer\n","optimizer = tf.keras.optimizers.Adam()\n","\n","#instantiate model\n","cnnmodel = ConvModel(layers_list, optimizer)\n","\n","train_losses, train_accuracies, test_losses, test_accuracies = training_loop(cnnmodel, train_dataset, test_dataset, epochs)\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["The first configuration had an accuracy score of about 75% after 15 epochs."]},{"cell_type":"markdown","metadata":{},"source":["## Visualization"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def visualization(train_losses, train_accuracies, test_losses, test_accuracies):\n","    ''' \n","    Visualizes training progress by showing losses and accuracies after every epoch\n","\n","    Args:\n","    train_losses --- loss during training \n","    train_accuracies -- accuracy during training \n","    test_losses --- loss during testing \n","    test_accuracies --- accuracy during testing \n","    '''\n","\n","    fig, axes = plt.subplots(nrows = 2, ncols = 1, sharex = True)\n","    axes[0].plot(train_losses, color = \"pink\", label = \"Training\")\n","    axes[0].plot(test_losses, color = \"lightblue\", label = \"Testing\")\n","    axes[1].plot(train_accuracies, color = \"pink\")\n","    axes[1].plot(test_accuracies, color = \"lightblue\")\n","\n","    axes[0].set_ylabel(\"Loss\"),\n","    axes[1].set_ylabel(\"Accuracy\")\n","    axes[1].set_xlabel(\"Training steps\")\n","\n","    plt.show()\n","\n","\n","\n"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mDer Kernel ist beim Ausführen von Code in der aktuellen Zelle oder einer vorherigen Zelle abgestürzt. Bitte überprüfen Sie den Code in der/den Zelle(n), um eine mögliche Fehlerursache zu identifizieren. Klicken Sie <a href='https://aka.ms/vscodeJupyterKernelCrash'>hier</a>, um weitere Informationen zu erhalten. Weitere Details finden Sie in Jupyter <a href='command:jupyter.viewOutput'>log</a>."]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mCanceled future for execute_request message before replies were done"]}],"source":["visualization(train_losses, train_accuracies, test_losses, test_accuracies)"]},{"cell_type":"markdown","metadata":{},"source":["## Experimenting with Hyperparameters and Architecture"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["#taking only a subset of data to speed up process\n","small_train_ds = train_ds.take(30000)\n","small_test_ds = test_ds.take(5000)\n","\n","small_train_dataset = small_train_ds.apply(prepare_cifar10_data)\n","small_test_dataset = small_test_ds.apply(prepare_cifar10_data)\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0: Accuracy 0.09799999743700027\n","Epoch 1: Accuracy 0.21819999814033508\n","Epoch 2: Accuracy 0.26899999380111694\n","Epoch 3: Accuracy 0.3246999979019165\n","Epoch 4: Accuracy 0.319599986076355\n"]}],"source":["layers_list = [tf.keras.layers.Conv2D(filters= 20, kernel_size=5, padding='same', activation='relu'),\n","                tf.keras.layers.Conv2D(filters=20, kernel_size=5, padding='same', activation='relu'),\n","                tf.keras.layers.Conv2D(filters=20, kernel_size=5, padding='same', activation='relu'),\n","                tf.keras.layers.MaxPooling2D(pool_size=2, strides=2),\n","                tf.keras.layers.Conv2D(filters=40, kernel_size=3, padding='same', activation='relu'),\n","                tf.keras.layers.Conv2D(filters=40, kernel_size=3, padding='same', activation='relu'),\n","                tf.keras.layers.GlobalAvgPool2D()]\n","\n","optimizer = tf.keras.optimizers.SGD()\n","epochs = 15\n","\n","new_model = ConvModel(layers_list, optimizer)\n","train_losses, train_accuracies, test_losses, test_accuracies = training_loop(new_model, small_train_dataset, small_test_dataset, epochs)\n","\n","                "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["visualization(train_losses, train_accuracies, test_losses, test_accuracies)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["layers_list = [tf.keras.layers.Conv2D(filters = 30 , kernel_size= 3, padding='same', activation='relu'),\n","             tf.keras.layers.Conv2D(filters= 30, kernel_size=3, padding='same', activation='relu'),\n","             tf.keras.layers.Conv2D(filters= 30, kernel_size=3, padding='same', activation='relu'),\n","             tf.keras.layers.GlobalAvgPool2D(),\n","             tf.keras.layers.Conv2D(filters = 60 , kernel_size= 3, padding='same', activation='relu'),\n","             tf.keras.layers.Conv2D(filters= 60, kernel_size=3, padding='same', activation='relu'),\n","             tf.keras.layers.GlobalAvgPool2D(),\n","             tf.keras.layers.Conv2D(filters = 120 , kernel_size= 3, padding='same', activation='softmax'),\n","             tf.keras.layers.GlobalAvgPool2D(),\n","             ]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["visualization(train_losses, train_accuracies, test_losses, test_accuracies)"]}],"metadata":{"kernelspec":{"display_name":"Python 3.7.13 ('iannwtf')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.13"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"1290a8c9e4f9d619e02c129e82eb2a58b00cf218ca683fba6d26250ac174955c"}}},"nbformat":4,"nbformat_minor":2}
