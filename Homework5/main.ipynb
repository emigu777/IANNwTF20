{"cells":[{"cell_type":"markdown","metadata":{},"source":[" # Homework 5 "]},{"cell_type":"markdown","metadata":{},"source":[" ### 2 . CIFAR10 - Classification "]},{"cell_type":"markdown","metadata":{},"source":["The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n","\n","Feature structure: \n","id: Text(dtype=object)\n","image: Image(shape=(32,32,3), dtype=uint8)\n","label: ClassLabel(dtype=int64, num_classes=10)"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import tensorflow_datasets as tfds\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","from matplotlib import image\n","import numpy as np\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["(train_ds, test_ds), ds_info = tfds.load('cifar10', split=['train', 'test'], as_supervised=True, with_info = True)\n","#print(ds_info)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def prepare_cifar10_data(cifar, batchsize = 32):\n","  '''\n","  This function prepares the dataset for usage in a learning model\n","\n","  Args:\n","  cifar --- The cifar-10 dataset \n","  batchsize --- The batch size (default:32)\n","\n","  Returns:\n","  cifar --- The processed dataset\n","\n","  '''\n","  #convert data from uint8 to float32 \n","  cifar = cifar.map(lambda img, target: (tf.cast(img, tf.float32), target))\n","  #sloppy input normalization, just bringing image values from range [0, 255] to [-1, 1]\n","  cifar = cifar.map(lambda img, target: ((img/128.)-1., target))\n","  #create one-hot targets\n","  cifar = cifar.map(lambda img, target: (img, tf.one_hot(target, depth=10)))\n","  #cache this progress in memory\n","  cifar = cifar.cache()\n","  #shuffle, batch, prefetch\n","  cifar = cifar.shuffle(1000)\n","  cifar = cifar.batch(batchsize)\n","  cifar= cifar.prefetch(tf.data.AUTOTUNE)\n","  #return preprocessed dataset\n","  return cifar\n","\n","\n","train_dataset = train_ds.apply(prepare_cifar10_data)\n","test_dataset = test_ds.apply(prepare_cifar10_data)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tfds.visualization.show_examples(test_ds, ds_info)"]},{"cell_type":"markdown","metadata":{},"source":["## Creating the Model"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["class ConvModel(tf.keras.Model):\n","    def __init__(self, layers_list, optimizer):\n","        super().__init__()\n","\n","        self.loss_function = tf.keras.losses.CategoricalCrossentropy()\n","        self.optimizer = optimizer\n","\n","        #instantiating layers through given list with layer objects\n","        self.con_layers = layers_list\n","        self.out = tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n","\n","        #metric objects to keep track of loss and accuracy\n","        self.metrics_list = [tf.keras.metrics.Mean(name=\"loss\"), tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\")]\n","\n","    @tf.function\n","    def call(self, input):\n","        '''\n","        Propagates input through network\n","\n","        Args:\n","        input --- the network's input\n","\n","        Returns:\n","        out --- the network's output\n","        '''\n","        out = self.con_layers[0](input)\n","        for layer in self.con_layers[1:]:\n","            out = layer(out)\n","        out = self.out(out)\n","        return out \n","\n","    #metrics property \n","    @property\n","    def metrics(self):\n","        # returns a list with all metrics in the model\n","        return self.metrics_list\n","\n","\n","    def reset_metrics(self):\n","        # reset all metrics objects\n","        for metric in self.metrics:\n","            metric.reset_states()\n","\n","\n","    @tf.function\n","    def train_step(self, input):\n","        '''\n","        Updates the trainable variables according to the calculated gradients \n","\n","        Args:\n","        input --- the input on which the training step is executed\n","\n","        Returns:\n","        A dictionary of loss and accuracy metrices \n","        '''\n","        img, target = input\n","        \n","        with tf.GradientTape() as tape:\n","            #create networks prediction\n","            output = self(img, training=True)\n","            loss = self.loss_function(target, output)\n","        #calculating gradients    \n","        gradients = tape.gradient(loss, self.trainable_variables)\n","        \n","        #applying gradients and updating model\n","        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n","        \n","        # update the state of the metrics according to loss\n","        self.metrics[0].update_state(loss)\n","        self.metrics[1].update_state(target, output)\n","\n","        \n","        # return a dictionary with metric names as keys and metric results as values\n","        return [m.result() for m in self.metrics]\n","\n","    @tf.function\n","    def test_step(self, data):\n","        ''' \n","        Tests how model performs on data it wasn't trained on \n","\n","        Args:\n","        data --- test data\n","\n","        Returns:\n","        A dictionary of loss and accuracy metrices \n","        '''\n","        img, target = data \n","\n","        #predict output and calculate loss\n","        output = self(img, training=True)\n","        loss = self.loss_function(target, output)\n","\n","        # update the state of the metrics according to loss\n","        self.metrics[0].update_state(loss)\n","        self.metrics[1].update_state(target, output)\n","\n","        return [m.result() for m in self.metrics]\n","\n","\n","\n","\n","\n","\n","        \n"]},{"cell_type":"markdown","metadata":{},"source":["## Training the network"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["def training_loop(model, train_data, test_data, epochs):\n","    ''' \n","    The training loop of our model\n","\n","    Args:\n","    model --- the model object \n","    epochs --- number of training steps\n","    '''\n","\n","    train_losses = []\n","    train_accuracies = []\n","    test_losses = []\n","    test_accuracies = []\n","\n","    #test metrics before training \n","    for data in test_data:\n","        test_loss, test_accuracy = model.test_step(data)\n","    test_losses.append(test_loss)\n","    test_accuracies.append(test_accuracy)\n","\n","    model.reset_metrics()\n","\n","    for epoch in range(epochs):\n","\n","        #print accuracy every epoch\n","        print(f'Epoch {str(epoch)}: Accuracy {test_accuracies[-1]}')\n","\n","        #train the network\n","        for data in train_data:\n","                train_loss, train_accuracy = model.train_step(data)\n","        train_losses.append(train_loss)\n","        train_accuracies.append(train_accuracy)\n","\n","        model.reset_metrics()\n","\n","        # test the network\n","        for data in test_dataset:\n","                test_loss, test_accuracy = model.test_step(data)\n","        test_losses.append(test_loss.numpy())\n","        test_accuracies.append(test_accuracy.numpy())\n","\n","        model.reset_metrics()\n","\n","    return train_losses, train_accuracies, test_losses, test_accuracies\n","\n"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0: Accuracy 0.09889999777078629\n"]}],"source":["#training for 15 epochs\n","epochs = 15\n","\n","#same layer config as in lecture \n","layers_list = [tf.keras.layers.Conv2D(filters=24, kernel_size=3, padding='same', activation='relu'),\n","        tf.keras.layers.Conv2D(filters=24, kernel_size=3, padding='same', activation='relu'), \n","        tf.keras.layers.MaxPooling2D(pool_size=2, strides=2),\n","        tf.keras.layers.Conv2D(filters=48, kernel_size=3, padding='same', activation='relu'),\n","        tf.keras.layers.Conv2D(filters=48, kernel_size=3, padding='same', activation='relu'),\n","        tf.keras.layers.GlobalAvgPool2D()]\n","\n","#using Adam optimizer\n","optimizer = tf.keras.optimizers.Adam()\n","\n","#instantiate model\n","cnnmodel = ConvModel(layers_list, optimizer)\n","\n","train_losses, train_accuracies, test_losses, test_accuracies = training_loop(cnnmodel, train_dataset, test_dataset, epochs)\n","\n","\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3.7.13 ('iannwtf')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.13"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"1290a8c9e4f9d619e02c129e82eb2a58b00cf218ca683fba6d26250ac174955c"}}},"nbformat":4,"nbformat_minor":2}
